# -*- coding: utf-8 -*-
"""ASR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GyhtHADoH5dV5DJYgQabjv06VTJ1CPFD

## I. Build dictionary
"""

from collections import defaultdict

def load_dictionary(path):
    dictionary = defaultdict(list)
    with open(path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                word = parts[0]
                phonemes = parts[1:]
                dictionary[word].append(phonemes)
    return dictionary

dictionary = load_dictionary('dictionary.txt')

"""## II. Build Word HMM

### Parse Phoneme HMM
"""

import re

def parse_hmm_file(file_path):
    phoneme_hmms = {}
    with open(file_path, "r") as f:
        lines = f.readlines()

    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith("~h"):
            phoneme_name = re.findall(r'"(.*?)"', line)[0]
            i += 1
            hmm = {"states": [], "transitions": [], "num_states": 0}

            while i < len(lines):
                line = lines[i].strip()
                if line.startswith("<NUMSTATES>"):
                    hmm["num_states"] = int(line.split()[1])
                    i += 1
                elif line.startswith("<STATE>"):
                    state_id = int(line.split()[1])
                    i += 1
                    mixtures = []
                    if lines[i].strip().startswith("<NUMMIXES>"):
                        num_mixes = int(lines[i].strip().split()[1])
                        i += 1
                        for _ in range(num_mixes):
                            if not lines[i].strip().startswith("<MIXTURE>"):
                                break
                            weight = float(lines[i].strip().split()[1])
                            i += 1
                            if lines[i].strip().startswith("<MEAN>"):
                                i += 1
                            mean = list(map(float, lines[i].strip().split()))
                            i += 1
                            if lines[i].strip().startswith("<VARIANCE>"):
                                i += 1
                            var = list(map(float, lines[i].strip().split()))
                            i += 1
                            mixtures.append({
                                "weight": weight,
                                "mean": mean,
                                "var": var
                            })
                    hmm["states"].append({"id": state_id, "mixtures": mixtures})
                elif line.startswith("<TRANSP>"):
                    trans_size = int(line.split()[1])
                    i += 1
                    for from_idx in range(trans_size):
                        row = list(map(float, lines[i].strip().split()))
                        for to_idx, prob in enumerate(row):
                            if prob > 0.0:
                                hmm["transitions"].append((from_idx, to_idx, prob))
                        i += 1
                elif line.startswith("<ENDHMM>"):
                    i += 1
                    break
                else:
                    i += 1
            phoneme_hmms[phoneme_name] = hmm
        else:
            i += 1

    return phoneme_hmms

phoneme_hmms = parse_hmm_file("hmm.txt")

"""### Build Word HMM"""

def build_word_hmm(word, phoneme_hmms, dictionary):
    states = []
    transitions = []
    state_offset = 0

    if word not in dictionary:
        raise ValueError(f"{word} is not in dictionary.")

    phoneme_seq = dictionary[word][0]  # 첫 번째 발음만 사용

    for phoneme in phoneme_seq:
        if phoneme not in phoneme_hmms:
            raise ValueError(f"Phoneme '{phoneme}' not in phoneme_hmms")

        hmm = phoneme_hmms[phoneme]

        # 상태 복사
        for state in hmm["states"]:
            states.append({
                "id": state_offset,
                "mixtures": state["mixtures"]
            })
            state_offset += 1

        # 전이 복사
        for from_s, to_s, prob in hmm["transitions"]:
            transitions.append((from_s + state_offset - len(hmm["states"]),
                                to_s + state_offset - len(hmm["states"]),
                                prob))

    return {
        "states": states,
        "transitions": transitions
    }

word_list = list(dictionary.keys())

word_hmms = {}
for word in word_list:
    word_hmms[word] = build_word_hmm(word, phoneme_hmms, dictionary)

"""## III. Build Universal HMM

### Parse Bigram
"""

def parse_bigram_file(file_path):
    bigram_probs = defaultdict(dict)

    with open(file_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) != 3:
                continue  # 잘못된 라인 무시
            prev, next_, prob = parts
            bigram_probs[prev][next_] = float(prob)

    return bigram_probs

bigram = parse_bigram_file("bigram.txt")

"""### Build Universal HMM"""

def build_universal_hmm(word_hmms, bigram, fill_missing=True, linear_prob=0.2):
    universal_hmm = {
        "states": [],
        "transitions": [],
        "word_segments": []
    }

    state_offset = 0
    word_start_state = {}
    word_end_state = {}

    # 1. 단어별 HMM 상태 및 내부 전이 연결
    for word, hmm in word_hmms.items():
        start_id = state_offset
        for state in hmm["states"]:
            universal_hmm["states"].append({
                "id": state_offset,
                "mixtures": state["mixtures"]
            })
            state_offset += 1
        end_id = state_offset - 1

        word_start_state[word] = start_id
        word_end_state[word] = end_id
        universal_hmm["word_segments"].append((word, start_id, end_id))

        for from_s, to_s, prob in hmm["transitions"]:
            universal_hmm["transitions"].append(
                (from_s + start_id, to_s + start_id, prob)
            )

    # 2. bigram 기반 단어 간 전이
    added_pairs = set()
    for prev_word, next_words in bigram.items():
        for next_word, prob in next_words.items():
            if prev_word in word_end_state and next_word in word_start_state:
                from_s = word_end_state[prev_word]
                to_s = word_start_state[next_word]
                universal_hmm["transitions"].append((from_s, to_s, prob))
                added_pairs.add((prev_word, next_word))

    # 3. 누락된 경우에만 선형 연결 보완
    if fill_missing:
        segments = universal_hmm["word_segments"]
        for i in range(len(segments) - 1):
            w1, end1, _ = segments[i]
            w2, start2, _ = segments[i + 1]

            if (w1, w2) not in added_pairs:
                universal_hmm["transitions"].append((end1, start2, linear_prob))

    return universal_hmm

universal_hmm = build_universal_hmm(word_hmms, bigram, fill_missing=True, linear_prob=0.2)

"""## IV. Viterbi Algorithm

### Load MFC
"""

import zipfile
import os

zip_path = "mfc.zip"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(".")

def load_mfc_file(filepath):
    with open(filepath, "r") as f:
        lines = f.readlines()

    header = lines[0].strip()
    T, D = map(int, header.split())
    data = []

    for line in lines[1:]:
        if line.strip():
            data.append(list(map(float, line.strip().split())))

    data = np.array(data)
    if data.shape != (T, D):
        raise ValueError(f"Shape mismatch: expected ({T}, {D}), got {data.shape}")
    return data

"""### Viterbi Decoding"""

import numpy as np

def gaussian_log_prob(x, mean, var):
    return -0.5 * np.sum(np.log(2 * np.pi * var) + ((x - mean) ** 2) / var)

def compute_emission_logprob(state, obs_vector):
    log_prob = -1e10
    for mix in state["mixtures"]:
        mean = np.array(mix["mean"])
        var = np.array(mix["var"])
        coeff = -0.5 * np.sum(np.log(var + 1e-10))
        dist = -0.5 * np.sum(((obs_vector - mean) ** 2) / (var + 1e-10))
        log_prob = np.logaddexp(log_prob, np.log(mix["weight"] + 1e-10) + coeff + dist)
    return log_prob

import math

def viterbi_decode(hmm, obs_seq, bigram, lambda1=1.0, lambda2=-5.0):
    N = len(hmm["states"])      # 상태 수
    T = len(obs_seq)            # 관측 벡터 길이
    log_zero = -1e10            # 로그 0 처리용

    # 상태 ID → 단어 매핑
    state_to_word = {}
    for word, start, end in hmm["word_segments"]:
        for sid in range(start, end + 1):
            state_to_word[sid] = word

    # 초기화
    delta = np.full((T, N), log_zero)
    psi = np.full((T, N), -1, dtype=int)

    # t = 0 (초기 상태 확률 + emission)
    for s in range(N):
        emission_score = compute_emission_logprob(hmm["states"][s], obs_seq[0])
        delta[0, s] = emission_score  # start prob = uniform or sil
        psi[0, s] = 0

    # Viterbi 업데이트
    for t in range(1, T):
        for s in range(N):
            max_prob = log_zero
            max_prev = -1
            curr_word = state_to_word.get(s, None)

            for from_s, to_s, trans_prob in hmm["transitions"]:
                if to_s != s:
                    continue

                prev_word = state_to_word.get(from_s, None)
                log_trans = math.log(trans_prob + 1e-10)

                if prev_word and curr_word:
                    lm_score = math.log(bigram.get(prev_word, {}).get(curr_word, 1e-6))
                    log_trans += lambda1 * lm_score

                    if prev_word != curr_word:
                        log_trans += lambda2

                score = delta[t - 1, from_s] + log_trans

                if score > max_prob:
                    max_prob = score
                    max_prev = from_s

            delta[t, s] = max_prob + compute_emission_logprob(hmm["states"][s], obs_seq[t])
            psi[t, s] = max_prev

    # 백트래킹
    best_path = []
    last_state = np.argmax(delta[T - 1])
    for t in reversed(range(T)):
        best_path.append(last_state)
        last_state = psi[t, last_state]
    best_path.reverse()

    return best_path

def decode_states_to_words(best_path, universal_hmm):
    word_segments = universal_hmm["word_segments"]
    word_map = {}

    for word, start, end in word_segments:
        for state_id in range(start, end + 1):
            word_map[state_id] = word

    decoded_words = []
    prev_word = None
    for state in best_path:
        word = word_map.get(state)
        if word is not None and word != prev_word:
            decoded_words.append(word)
            prev_word = word

    return decoded_words

def decode_state_sequence_to_words(state_sequence, word_segments):
    result = []
    added_words = set()
    for sid in state_sequence:
        for word, start, end in word_segments:
            if start <= sid <= end and word not in added_words:
                result.append(word)
                added_words.add(word)
                break
    return result

import glob
import os

def run_batch_viterbi(universal_hmm, mfc_root="mfc", output_file="recognized.txt", lambda1=1.0, lambda2=-5.0):
    file_list = glob.glob(os.path.join(mfc_root, "*", "*", "*.txt"))
    print(f"📁 총 {len(file_list)}개 파일 디코딩 시작...\n")

    results = []

    for path in file_list:
        try:
            # ① 관측 벡터 로드
            obs = load_mfc_file(path)

            # ② Viterbi 디코딩
            best_path = viterbi_decode(universal_hmm, obs, bigram, lambda1=lambda1, lambda2=lambda2)
            decoded_words = decode_states_to_words(best_path, universal_hmm)

            # ③ 상대 경로 기반 라벨 경로 계산
            relative_path = os.path.relpath(path, mfc_root)
            lab_path = os.path.splitext(relative_path)[0] + ".rec"
            label_header = f"\"{os.path.join(mfc_root, lab_path)}\""

            # ④ 결과 누적
            result_block = [label_header] + decoded_words + ["."]
            results.append("\n".join(result_block))

            print(f"✅ {relative_path} → 상태 수: {len(best_path)}  |  단어: {' '.join(decoded_words)}")

        except Exception as e:
            print(f"{path} 처리 실패: {e}")

    # ⑤ recognized.txt 하나로 저장
    with open(output_file, "w") as f:
        f.write("\n".join(results) + "\n")

lambda1 = 0.9
lambda2 = -5

output_filename = f"recognized_l1{lambda1}_l2{lambda2}.txt"

print(f"Running Viterbi with λ1 = {lambda1}, λ2 = {lambda2} → {output_filename}")

run_batch_viterbi(
    universal_hmm,
    mfc_root="mfc",
    output_file=output_filename,
    lambda1=lambda1,
    lambda2=lambda2
)

"""### Save Recognized"""

def sort_recognized_file(input_path="recognized.txt", output_path="recognized.txt"):
    with open(input_path, "r", encoding="utf-8") as f:
        lines = f.read().strip().split("\n")

    blocks = []
    block = []

    for line in lines:
        line = line.strip()
        if line.startswith("\"") and block:
            blocks.append(block)
            block = [line.replace(".lab", ".rec")]  # 확장자 변경
        else:
            block.append(line)
    if block:
        blocks.append(block)

    # 헤더 기준 정렬
    blocks.sort(key=lambda x: x[0])  # "mfc/..." 기준

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("#!MLF!#\n")

        for block in blocks:
            for idx, line in enumerate(block):
                if idx == 0:
                    f.write(line + "\n")
                else:
                    cleaned = line.replace("<s>", "").strip()
                    if cleaned:
                        f.write(cleaned + "\n")

from google.colab import files

filename = f"recognized_l1{lambda1}_l2{lambda2}.txt"
#filename = "recognized.txt"

sort_recognized_file(
    input_path=filename,
    output_path=filename
)

try:
    files.download(filename)
except Exception as e:
    print(f"Failed to download {filename}: {e}")